---
layout: post
title: Babi
---

The Babi dataset is a set of 20 toy tasks for the question-answering problem in natural language understanding. Each task consists of a type of stories, followed by a `(question, answer, supporting-line-number)` tuple which forms the basis for training and validation. 

Typically, the network is trained on each task, and the performance is reported on each task. 
Loosely following the approach in [this Google paper][OneModel], it appears that a single network that has been trained on all 20 tasks performs significantly better on the *hard* tasks than a network trained only on examples from that task. And the performance penalty on the *easy* tasks isn't too great. 

Training is limited to 20 epochs to mimic what is done on the [Facebook research paper][FacebookBabi], and on the [Keras example][Keras]. 
The network is far from fully trained, however, so there is significant scope for improving these numbers just by training longer. 
The training tasks are created using the following script, so the training size for each particular task is still only 1000. 

```bash
#!/bin/bash

for j in `seq 1 2` ; do  
    for i in `seq -f %02g 1 20` ; do
        if [ "$j" -lt "2" ]; then
            append=train
        else
            append=test
        fi
        fname=task_${i}_${j}_${append}
        babi-tasks ${i} 1000 > ${fname}
        printf "                      \r%s\r" "${fname}"
    done
done

tar -cvf tasks.tar task_*_*_*
rm task_*_*_*

```

The performance of the simultaneously trained network is presented in this table. The _FB LSTM Baseline_ is the performance presented in the original Facebook paper. The _Keras_ performance is what is presented on the Keras blog.


Task Set   |  Keras All at Once | FB LSTM Baseline | Keras |
:----:     |:----:              |:----:            |:----: |
01	|	100.0000         | 50               | 100.0      |   
02	|	72.1800          | 20               | 50.0       | 
03	|	69.0800          | 20               | 20.5       |  
04	|	71.3000          | 61               | 62.9       |  
05	|	92.8000          | 70               | 61.9       |  
06	|	80.8400          | 48               | 50.7       |  
07	|	97.0600          | 49               | 78.9       |  
08	|	98.6200          | 45               | 77.2       |  
09	|	85.8800          | 64               | 64.0       |  
10	|	83.2200          | 44               | 47.7       |  
11	|	100.0000         | 72               | 74.9       |  
12	|	99.9800          | 74               | 76.4       |  
13	|	100.0000         | 94               | 94.4       |  
14	|	80.4600          | 27               | 34.8       |  
15	|	50.0500          | 21               | 32.4       |  
16	|	37.8000          | 23               | 50.6       |  
17	|	51.1125          | 51               | 49.1       |  
18	|	88.4391          | 52               | 90.8       |  
19	|	8.7000           | 8                | 9.0        |
20	|	96.5417          | 91               | 90.7       |
{:.mbtablestyle}  


## Code

The code is available here, and builds extensively on the example on the Keras github. 
The main difference is that instead of downloading the Babi examples from the facebook AWS instance, I generated them using the babi-tasks repo on the [Facebook Research github][FacebookGithub]. 

Once you have installed the requirements to run `babi-tasks`, the script `make-tasks.sh` generates a training and test file with all 20 tasks. The `extract_with_token` helper function extracts (and optionally flattens) the tasks so that training can be done on all the tasks all at once, while the testing can be done on a task-by-task basis. 



## References
1. [One Model To Learn Them All][OneModel]
2. [Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks][FacebookBabi]
3. [Keras][Keras]
4. [Facebook Github][FacebookGithub]


[OneModel]: https://arxiv.org/abs/1706.05137
[FacebookBabi]: https://arxiv.org/abs/1502.05698
[Keras]: https://keras.io/
[FacebookGithub]: https://github.com/facebook/bAbI-tasks






